{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertPreTrainedModel(PreTrainedModel):\n",
    "#     \"\"\"\n",
    "#     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "#     models.\n",
    "#     \"\"\"\n",
    "\n",
    "#     config_class = BertConfig\n",
    "#     load_tf_weights = load_tf_weights_in_bert\n",
    "#     base_model_prefix = \"bert\"\n",
    "#     authorized_missing_keys = [r\"position_ids\"]\n",
    "\n",
    "#     def _init_weights(self, module):\n",
    "#         \"\"\" Initialize the weights \"\"\"\n",
    "#         if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "#             # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "#             # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "#         elif isinstance(module, nn.LayerNorm):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1.0)\n",
    "#         if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "#             module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.empty(512, 128)\n",
    "w = nn.init.normal_(w, mean=0.0, std=0.02)\n",
    "# w = nn.init.xavier_normal_(w, gain=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0005e-05), tensor(0.0200))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.view(-1).mean(), w.view(-1).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pre-defined model (only architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-4_H-128_A-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_config = AutoConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = AutoModel.from_config(temp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 128])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.encoder.layer[0].intermediate.dense.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.217758e-05 0.019956263\n",
      "4.0291143e-05 0.019954162\n",
      "-2.8450202e-05 0.01999053\n",
      "-3.6296515e-05 0.020029351\n"
     ]
    }
   ],
   "source": [
    "# module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "for layer_idx in range(temp_config.num_hidden_layers):\n",
    "    temp_layer_weight = temp_model.encoder.layer[layer_idx].intermediate.dense.weight.detach().numpy()\n",
    "#     temp_layer_bias = temp_model.encoder.layer[layer_idx].intermediate.dense.bias.detach().numpy()\n",
    "    print(temp_layer_weight.flatten().mean(), temp_layer_weight.flatten().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pretrained_model = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-128_A-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00025537418 0.06918144\n",
      "-0.00038769463 0.07180514\n",
      "-0.0006810591 0.07812271\n",
      "-0.0002975062 0.0737309\n"
     ]
    }
   ],
   "source": [
    "for layer_idx in range(temp_config.num_hidden_layers):\n",
    "    temp_layer_weight = temp_pretrained_model.encoder.layer[layer_idx].intermediate.dense.weight.detach().numpy()\n",
    "#     temp_layer_bias = temp_pretrained_model.encoder.layer[layer_idx].intermediate.dense.bias.detach().numpy()\n",
    "    print(temp_layer_weight.flatten().mean(), temp_layer_weight.flatten().std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
