## model

### Transformer

##### Must Have Read :star:

- [x] Attention Is All You Need (NIPS 2017) [[paper]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
  - [Illustrated-Transformer](http://jalammar.github.io/illustrated-transformer/)
  - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[paper]](https://arxiv.org/abs/1810.04805)

##### Understanding the Architecture

- [ ] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (ACL 2019) [[paper]](https://arxiv.org/pdf/1905.09418.pdf)
- [ ] Are Sixteen Heads Really Better than One? (NIPS 2019) [[paper]](https://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one.pdf)

##### Variants

- [ ] Longformer: The Long-Document Transformer [[paper]](https://arxiv.org/abs/2004.05150)
- [x] Big Bird: Transformers for Longer Sequences [[paper]](https://arxiv.org/abs/2007.14062)
  - Longformer + Random Attention (random walk on multiple layers)

<br/>

## task

### Question Answering

#### QA on Knowledge Graph

##### Survey :star:

- [x] Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs [[paper]](https://arxiv.org/pdf/1907.09361.pdf)

<br/>

- [ ] Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks [[paper]](https://www.aclweb.org/anthology/N18-2047.pdf)

#### QA on Table

- [ ] Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs (EMNLP 2019)
- [x] TAPAS: Weakly Supervised Table Parsing via Pre-training (ACL 2020)
- [ ] TABERT: Pretraining for Joint Understanding of Textual and Tabular Data

<br/>

<br/>

<br/>

---

#### Self-Supervised Learning

##### Contrastive Learning

- [ ] DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations [[paper]](https://arxiv.org/pdf/2006.03659.pdf)

<br/>

<br/>

<br/>


### Neural ODEs
- [ ] Neural Ordinary Differential Equations (NIPS 2018) [[paper]](https://arxiv.org/pdf/1806.07366.pdf)
- [ ] Hypersolvers: Toward Fast Continuous-Depth Models [[paper]](https://arxiv.org/abs/2007.09601)

<br/>

<br/>

<br/>

### Program Induction
- [x] NEURAL PROGRAMMER-INTERPRETERS (ICLR 2016) [[paper]](https://arxiv.org/pdf/1511.06279.pdf)
- [x] NEURAL PROGRAMMER: INDUCING LATENT PROGRAMS WITH GRADIENT DESCENT (ICLR 2016) [[paper]](https://arxiv.org/pdf/1511.04834.pdf)
- [ ] Neural Program Meta-Induction (NIPS 2017) [[paper]](https://papers.nips.cc/paper/6803-neural-program-meta-induction.pdf)

<br/>

<br/>

<br/>



ETC

- [ ] Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs (TACL 2019) [[paper]](https://www.aclweb.org/anthology/Q19-1012.pdf)
- [ ] Neuro-Symbolic Program Synthesis (ICLR 2017) [[paper]](https://arxiv.org/pdf/1611.01855.pdf)
- [ ] Neural Module Networks (CVPR 2016) [[paper]](https://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf)
- [ ] Cross-lingual Language Model Pretraining (NIPS 2019)

